version: '3'

# This docker-compose is for developer convenience, not for running in production.

services:

  yarn-resourcemanager:
    image: ghcr.io/kbase/cdm-prototype-yarn:pr-8
    container_name: yarn-resourcemanager
    # Images from the ghcr.io/kbase registry are exclusively available for Linux/AMD64 platforms.
    platform: linux/amd64
    ports:
      - 8088:8088  # web ui
    environment:
      - YARN_MODE=resourcemanager
      - MINIO_URL=http://minio:9002
      - MINIO_ACCESS_KEY=yarnuser
      - MINIO_SECRET_KEY=yarnpass

  yarn-nodemanager:
    image: ghcr.io/kbase/cdm-prototype-yarn:pr-8
    container_name: yarn-nodemanager
    platform: linux/amd64
    ports:
      - 8042:8042  # web ui
    environment:
      - YARN_MODE=nodemanager
      - YARN_RESOURCEMANAGER_HOSTNAME=yarn-resourcemanager
      - MINIO_URL=http://minio:9002
      - MINIO_ACCESS_KEY=yarnuser
      - MINIO_SECRET_KEY=yarnpass

  spark-master:
    # The latest image from cdm-jupyterhub that includes spark standalone mode
    image: ghcr.io/kbase/cdm-jupyterhub:pr-74
    container_name: spark-master
    platform: linux/amd64
    ports:
      - "8090:8090"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_WEBUI_PORT=8090
      - MAX_EXECUTORS=4
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
      - POSTGRES_URL=postgres:5432
    volumes:
      - ./cdr/cdm/jupyter:/cdm_shared_workspace

  spark-worker-1:
    # The latest image from cdm-jupyterhub that includes spark standalone mode
    image: ghcr.io/kbase/cdm-jupyterhub:pr-74
    container_name: spark-worker-1
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_WEBUI_PORT=8081
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
      - POSTGRES_URL=postgres:5432
    volumes:
      - ./cdr/cdm/jupyter:/cdm_shared_workspace

  spark-worker-2:
    # The latest image from cdm-jupyterhub that includes spark standalone mode
    image: ghcr.io/kbase/cdm-jupyterhub:pr-74
    container_name: spark-worker-2
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - "8082:8082"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_WEBUI_PORT=8082
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
      - POSTGRES_URL=postgres:5432
    volumes:
      - ./cdr/cdm/jupyter:/cdm_shared_workspace

  minio:
    image: minio/minio
    container_name: spark-minio
    ports:
      - "9002:9002"
      # MinIO Console is available at http://localhost:9003
      - "9003:9003"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    healthcheck:
      # reference: https://github.com/rodrigobdz/docker-compose-healthchecks?tab=readme-ov-file#minio-release2023-11-01t18-37-25z-and-older
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9002' || exit 1
      interval: 1s
      timeout: 10s
      retries: 5
    # Note there is no bucket by default
    command: server --address 0.0.0.0:9002 --console-address 0.0.0.0:9003 /data

  minio-create-bucket:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /scripts/minio_create_bucket_entrypoint.sh
    volumes:
      - ./config/cdm-read-only-policy.json:/config/cdm-read-only-policy.json
      - ./config/cdm-read-write-policy.json:/config/cdm-read-write-policy.json
      - ./config/yarn-write-policy.json:/config/yarn-write-policy.json
      - ./scripts/minio_create_bucket_entrypoint.sh:/scripts/minio_create_bucket_entrypoint.sh

  dev_jupyterlab:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: dev-jupyterlab
    ports:
      - "4041:4041"
    depends_on:
      - spark-master
      - minio-create-bucket
    environment:
      - NOTEBOOK_PORT=4041
      - JUPYTER_MODE=jupyterlab
      - YARN_RESOURCE_MANAGER_URL=http://yarn-resourcemanager:8032
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_DRIVER_HOST=dev-jupyterlab
      - MINIO_URL=http://minio:9002
      - MINIO_ACCESS_KEY=minio-readwrite
      - MINIO_SECRET_KEY=minio123
      - S3_YARN_BUCKET=yarn
      - MAX_EXECUTORS=4
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
      - POSTGRES_URL=postgres:5432
      - USAGE_MODE=dev  # Enabling dev mode grants full access to MinIO and additional privileges for services like Hive, such as the ability to create tables as defined in the scripts/setup.sh.
    volumes:
      - ./cdr/cdm/jupyter:/cdm_shared_workspace

  user-jupyterlab:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: user-jupyterlab
    ports:
      - "4042:4042"
    depends_on:
      - spark-master
      - minio-create-bucket
    environment:
      - NOTEBOOK_PORT=4042
      - JUPYTER_MODE=jupyterlab
      - YARN_RESOURCE_MANAGER_URL=http://yarn-resourcemanager:8032
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_DRIVER_HOST=user-jupyterlab
      - MINIO_URL=http://minio:9002
      - MINIO_ACCESS_KEY=minio-readonly
      - MINIO_SECRET_KEY=minio123
      - S3_YARN_BUCKET=yarn
      - MAX_EXECUTORS=4
      # TODO: create postgres user w/ only write access to the hive tables
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
      - POSTGRES_URL=postgres:5432
    volumes:
      - ./cdr/cdm/jupyter/user_shared_workspace:/cdm_shared_workspace/user_shared_workspace

  dev_jupyterhub:
      build:
        context: .
        dockerfile: Dockerfile
      container_name: dev-jupyterhub
      ports:
        - "4043:4043"
      depends_on:
        - spark-master
        - minio-create-bucket
      environment:
        - NOTEBOOK_PORT=4043
        - JUPYTER_MODE=jupyterhub
        - YARN_RESOURCE_MANAGER_URL=http://yarn-resourcemanager:8032
        - SPARK_MASTER_URL=spark://spark-master:7077
        - SPARK_DRIVER_HOST=dev-jupterhub
        - MINIO_URL=http://minio:9002
        - MINIO_ACCESS_KEY=minio-readwrite
        - MINIO_SECRET_KEY=minio123
        - S3_YARN_BUCKET=yarn
        - MAX_EXECUTORS=4
        - POSTGRES_USER=hive
        - POSTGRES_PASSWORD=hivepassword
        - POSTGRES_DB=hive
        - POSTGRES_URL=postgres:5432
        - USAGE_MODE=dev
      volumes:
        - ./cdr/cdm/jupyter:/cdm_shared_workspace
        - ./cdr/cdm/jupyter/home:/home

  user_jupyterhub:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: user-jupyterhub
    ports:
      - "4044:4044"
    depends_on:
      - spark-master
      - minio-create-bucket
    environment:
      - NOTEBOOK_PORT=4044
      - JUPYTER_MODE=jupyterhub
      - YARN_RESOURCE_MANAGER_URL=http://yarn-resourcemanager:8032
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_DRIVER_HOST=user-jupyterhub
      - MINIO_URL=http://minio:9002
      - MINIO_ACCESS_KEY=minio-readonly
      - MINIO_SECRET_KEY=minio123
      - S3_YARN_BUCKET=yarn
      - JUPYTER_MODE=jupyterhub
      - MAX_EXECUTORS=4
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
      - POSTGRES_URL=postgres:5432
    volumes:
      - ./cdr/cdm/jupyter/home:/home

  postgres:
    image: postgres:16.3
    restart: always
    container_name: postgres
    # To avoid incorrect user permissions, manually create the volume directory before running Docker.
    # export UID=$(id -u)
    # export GID=$(id -g)
    # mkdir -p cdr/cdm/jupyter/cdm-postgres
    # reference: https://forums.docker.com/t/systemd-coredump-taking-ownership-of-tmp-db-directory-and-contents-in-rails-app/93609
    user: "${UID}:${GID}"
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hivepassword
      - POSTGRES_DB=hive
    volumes:
      - ./cdr/cdm/jupyter/cdm-postgres:/var/lib/postgresql/data  # For local development only. In Rancher development, PostgreSQL data shouldn't be stored in a shared mount.